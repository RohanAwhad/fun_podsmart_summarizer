{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import Together\n",
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt_template = (\n",
    "  'Write a concise summary for text enclosed in triple backticks (```)\\n'\n",
    "  '- Output the summary in bullet points.\\n'\n",
    "  '- Each point should contain no more than 2 sentences.\\n'\n",
    "  '- Keep the summary concise.'\n",
    "  '- Points should not contain redundant information.\\n'\n",
    "  '- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\\n'\n",
    "  '- Summary should be grammatically correct.\\n'\n",
    "  '\\n'\n",
    "  '```\\n'\n",
    "  '{text}\\n'\n",
    "  '```\\n'\n",
    "  '\\n'\n",
    "  'Return your answer in the following format:\\n'\n",
    "  'Title: <title>\\n'\n",
    "  'Summary:\\n'\n",
    "  '- <summary point 1>\\n'\n",
    "  '- <summary point 2>\\n'\n",
    "  '- <summary point n>\\n'\n",
    "  'e.g.\\n'\n",
    "  'Title: Why Artificial Intelligence is Good\\n'\n",
    "  'Summary:\\n'\n",
    "  '- AI can make humans more productive by automating many repetitive processes.\\n'\n",
    "  '\\n'\n",
    "  '---\\n'\n",
    "  'TITLE AND CONCISE SUMMARY:\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "```\n",
      "{text}\n",
      "```\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "---\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(map_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site ...\n"
     ]
    }
   ],
   "source": [
    "text = '''Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally -- wisely -- left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn't let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they're all still there, and Larry is not. Micro-managing isn't that third thing that Amazon does better than us, by the way. I mean, yeah, they micro-manage really well, but I wouldn't list it as a strength or anything. I'm just trying to set the context here, to help you understand what happened. We're talking about a guy who in all seriousness has said on many public occasions that people should be paying him to work at Amazon. He hands out little yellow stickies with his name on them, reminding people \"who runs the company\" when they disagree with him. The guy is a regular... well, Steve Jobs, I guess. Except without the fashion or design sense. Bezos is super smart; don't get me wrong. He just makes ordinary control freaks look like stoned hippies.'''\n",
    "print(text[:100], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = map_prompt_template.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Jeff Bezos' Management Style at Amazon\n",
      "\n",
      "Summary:\n",
      "- Jeff Bezos is known for his intensive micro-management, particularly concerning Amazon's retail website design.\n",
      "- Bezos hired and ignored the advice of renowned human-computer interaction expert Larry Tesler, leading to Tesler's eventual departure from Amazon.\n",
      "- Despite usability studies showing the website's complexity, Bezos maintained control over the site's detailed design.\n",
      "- The narrative suggests that while Amazon is adept at micro-managing, it is not considered a company strength.\n",
      "- Bezos is characterized as having an ego, exemplified by distributing reminders of his leadership and suggesting people should pay to work at Amazon.\n",
      "- The text compares Bezos to Steve Jobs, noting Bezos's intelligence but implying he lacks Jobs's sense of fashion and design.\n"
     ]
    }
   ],
   "source": [
    "prompt = map_prompt_template.format(text=text)\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "res = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[{'role': 'user', 'content': prompt}],\n",
    "  temperature=0.4,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input:\n",
      "---\n",
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "```\n",
      "Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally -- wisely -- left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn't let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they're all still there, and Larry is not. Micro-managing isn't that third thing that Amazon does better than us, by the way. I mean, yeah, they micro-manage really well, but I wouldn't list it as a strength or anything. I'm just trying to set the context here, to help you understand what happened. We're talking about a guy who in all seriousness has said on many public occasions that people should be paying him to work at Amazon. He hands out little yellow stickies with his name on them, reminding people \"who runs the company\" when they disagree with him. The guy is a regular... well, Steve Jobs, I guess. Except without the fashion or design sense. Bezos is super smart; don't get me wrong. He just makes ordinary control freaks look like stoned hippies.\n",
      "```\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "---\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "\n",
      "---\n",
      "Output:\n",
      "---\n",
      "Title: Jeff Bezos' Management Style at Amazon\n",
      "\n",
      "Summary:\n",
      "- Jeff Bezos is known for his intensive micro-management, particularly of Amazon's retail website's design.\n",
      "- Bezos hired Larry Tesler, a highly respected human-computer interaction expert, and disregarded his advice, leading to Tesler's departure.\n",
      "- Despite usability studies showing the website's complexity, Bezos maintained control over the design, likening the pixels to his children.\n",
      "- Bezos is recognized for his extreme attention to detail and control within the company, often reminding employees of his leadership with personalized sticky notes.\n",
      "- Although Bezos is acknowledged for his intelligence, the text criticizes his lack of fashion or design sense and compares his management to an extreme version of typical control freaks.\n"
     ]
    }
   ],
   "source": [
    "prompt = map_prompt_template.format(text=text)\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "res = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[{'role': 'user', 'content': prompt}],\n",
    "  temperature=0.4,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "together_client = OpenAI(\n",
    "  api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
    "  base_url='https://api.together.xyz',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Jeff Bezos' Management Style\n",
      "\n",
      "Summary:\n",
      "- Jeff Bezos, CEO of Amazon, is known for meticulous micro-management.\n",
      "- He reportedly ignored usability advice from a respected expert, prioritizing control over functionality.\n",
      "- Bezos is described as highly intelligent but a control freak, comparable to Steve Jobs without the design sense.\n",
      "- He uses yellow sticky notes to assert his authority and has publicly suggested people should pay him for the privilege of working at Amazon.\n"
     ]
    }
   ],
   "source": [
    "prompt = map_prompt_template.format(text=text)\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "llama2_70_res = together_client.chat.completions.create(\n",
    "  model=\"togethercomputer/llama-2-70b-chat\",\n",
    "  messages=[{'role': 'user', 'content': map_prompt_template.format(text=text)}],\n",
    "  temperature=0.4,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(llama2_70_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Title: Jeff Bezos' Micro-Managing Style\n",
      "\n",
      "Summary:\n",
      "\n",
      "* Jeff Bezos is known for his extreme micro-managing style, even ignoring expert advice from the likes of Apple's Chief Scientist, Larry Tesler.\n",
      "* Bezos is deeply involved in every aspect of Amazon's retail site, including the design and layout, which he personally reviews and approves.\n",
      "* His micro-managing has led to a culture at Amazon where employees are afraid to disagree with him, and he has been known to hand out yellow stickies with his name on them to remind people who runs the company.\n",
      "* Bezos is highly intelligent, but his extreme control freak nature has led some to compare him to Steve Jobs, minus the fashion sense.\n"
     ]
    }
   ],
   "source": [
    "prompt = map_prompt_template.format(text=text)\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "llama2_70_res = together_client.chat.completions.create(\n",
    "  model=\"togethercomputer/llama-2-70b-chat\",\n",
    "  messages=[{'role': 'user', 'content': map_prompt_template.format(text=text)}],\n",
    "  temperature=0.4,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(llama2_70_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input:\n",
      "---\n",
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "```\n",
      "Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally -- wisely -- left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn't let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they're all still there, and Larry is not. Micro-managing isn't that third thing that Amazon does better than us, by the way. I mean, yeah, they micro-manage really well, but I wouldn't list it as a strength or anything. I'm just trying to set the context here, to help you understand what happened. We're talking about a guy who in all seriousness has said on many public occasions that people should be paying him to work at Amazon. He hands out little yellow stickies with his name on them, reminding people \"who runs the company\" when they disagree with him. The guy is a regular... well, Steve Jobs, I guess. Except without the fashion or design sense. Bezos is super smart; don't get me wrong. He just makes ordinary control freaks look like stoned hippies.\n",
      "```\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "---\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "\n",
      "---\n",
      "Output:\n",
      "---\n",
      "Title: Jeff Bezos' Micro-Management Style at Amazon\n",
      "\n",
      "Summary:\n",
      "- Jeff Bezos, CEO of Amazon, is known for meticulous micro-management, overseeing every detail of the company's retail site.\n",
      "- Bezos hired usability expert Larry Tesler, but reportedly disregarded his advice for three years, leading to Tesler's departure.\n",
      "- Amazon's website reflects this micro-management, with millions of pixels of information, making it complex to navigate.\n",
      "- Bezos is described as having a controlling nature, often using yellow sticky notes to assert his authority and compare himself to Steve Jobs in terms of control.\n",
      "- Despite his controlling tendencies, Bezos is recognized for his intelligence and ability to make Amazon a successful company.\n"
     ]
    }
   ],
   "source": [
    "prompt = map_prompt_template.format(text=text)\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "mistral_res = together_client.chat.completions.create(\n",
    "  model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "  messages=[{'role': 'user', 'content': map_prompt_template.format(text=text)}],\n",
    "  temperature=0.4,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(mistral_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input:\n",
      "---\n",
      "<s> [INST]\n",
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "```\n",
      "Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally -- wisely -- left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn't let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they're all still there, and Larry is not. Micro-managing isn't that third thing that Amazon does better than us, by the way. I mean, yeah, they micro-manage really well, but I wouldn't list it as a strength or anything. I'm just trying to set the context here, to help you understand what happened. We're talking about a guy who in all seriousness has said on many public occasions that people should be paying him to work at Amazon. He hands out little yellow stickies with his name on them, reminding people \"who runs the company\" when they disagree with him. The guy is a regular... well, Steve Jobs, I guess. Except without the fashion or design sense. Bezos is super smart; don't get me wrong. He just makes ordinary control freaks look like stoned hippies.\n",
      "```\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "---\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "[/INST]\n",
      "---\n",
      "Output:\n",
      "---\n",
      "TITLE: Jeff Bezos' Management Style at Amazon\n",
      "\n",
      "Summary:\n",
      "- Jeff Bezos is known for meticulous micro-management, particularly of Amazon's retail site.\n",
      "- Bezos reportedly ignored usability advice from Larry Tesler, a renowned human-computer interaction expert.\n",
      "- Amazon's landing page retains numerous pixels of information, despite usability issues.\n",
      "- Bezos has been compared to Steve Jobs for his control-freak tendencies and use of yellow sticky notes to assert authority.\n",
      "- Despite these traits, Bezos is recognized for his intelligence and ability to make humans more productive through technology.\n"
     ]
    }
   ],
   "source": [
    "prompt = '''<s> [INST]\n",
    "Write a concise summary for text enclosed in triple backticks (```)\n",
    "- Output the summary in bullet points.\n",
    "- Each point should contain no more than 2 sentences.\n",
    "- Keep the summary concise.- Points should not contain redundant information.\n",
    "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
    "- Summary should be grammatically correct.\n",
    "\n",
    "```\n",
    "Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally -- wisely -- left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn't let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they're all still there, and Larry is not. Micro-managing isn't that third thing that Amazon does better than us, by the way. I mean, yeah, they micro-manage really well, but I wouldn't list it as a strength or anything. I'm just trying to set the context here, to help you understand what happened. We're talking about a guy who in all seriousness has said on many public occasions that people should be paying him to work at Amazon. He hands out little yellow stickies with his name on them, reminding people \"who runs the company\" when they disagree with him. The guy is a regular... well, Steve Jobs, I guess. Except without the fashion or design sense. Bezos is super smart; don't get me wrong. He just makes ordinary control freaks look like stoned hippies.\n",
    "```\n",
    "\n",
    "Return your answer in the following format:\n",
    "Title: <title>\n",
    "Summary:\n",
    "- <summary point 1>\n",
    "- <summary point 2>\n",
    "- <summary point n>\n",
    "e.g.\n",
    "Title: Why Artificial Intelligence is Good\n",
    "Summary:\n",
    "- AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "---\n",
    "TITLE AND CONCISE SUMMARY:\n",
    "[/INST]'''\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "mistral_res = together_client.chat.completions.create(\n",
    "  model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "  messages=[{'role': 'user', 'content': prompt}],\n",
    "  temperature=0.4,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(mistral_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: It seems to havae good recall, and is able to retain a lot of information while summarizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input:\n",
      "---\n",
      "<s> [INST]\n",
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "```\n",
      "LLMs like GPT-4 have taken the world by storm, and one of the tasks that generative text models are particularly good at is summarization of long texts such as books or podcast transcripts. However, the conventional method of getting LLMs to summarize long texts is actually fundamentally flawed. In this post, I will tell you about the problems with existing summarization methods, and present a better summarization method that actually takes into account the structure of the text! Even better, this method will also give us the text’s main topics — killing two birds with one stone!I will walk you through how you can easily implement this in Python, with just several tweaks of the existing method. This is the method that we use at Podsmart, our newly-launched AI-powered podcast summarizer app that helps busy intellectuals save hours of listening.Problems with existing solutionsThe canonical method to summarize long texts is by recursive summarization, in which the long text is split equally into shorter chunks which can fit inside the LLM’s context window. Each chunk is summarized, and the summaries are concatenated together to and then passed through GPT-3 to be further summarized. This process is repeated until one obtains a final summary of desired length.However, the major downside is that existing implementations e.g. LangChain’s summarize chain using map_reduce, split the text into chunks with no regard for the logical and structural flow of the text.For example, if the article is 1000 words long, a chunk size of 200 would mean that we would get 5 chunks. What if the author has several main points, the first of which takes up the first 250 words? The last 50 words would be placed into the second chunk with text from the author’s next point, and passing this chunk through GPT-3’s summarizer would lead to potentially important information from the first point being omitted. Also, some key points may be longer than others, and there is no way of knowing this a priori.Another method is the ‘refine’ method, which passes every chunk of text, along with a summary from previous chunks, through the LLM, which progressively refines the summary as it sees more of the text (see the prompt here). However, the sequential nature of the process means that it cannot be parallelized and takes linear time, far longer than a recursive method which takes logarithmic time. Additionally, intuition suggests that the meaning from the initial parts will be overrepresented in the final summary. For podcast transcripts where the first minutes are advertisements completely irrelevant to the rest of the podcast, this is a stumbling block. Hence, this method is hence not widely used.\n",
      "```\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "---\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "[/INST]\n",
      "---\n",
      "Output:\n",
      "---\n",
      "Title: A Superior Summarization Method for Long Texts Using LLMs\n",
      "\n",
      "Summary:\n",
      "- The conventional recursive summarization method for long texts using LLMs, such as GPT-3, splits texts into equal chunks without considering the text's logical and structural flow, leading to potential loss of important information.\n",
      "- The 'refine' method, which progressively refines the summary as it sees more of the text, cannot be parallelized and takes linear time, making it less efficient. Additionally, it may overrepresent the meaning from the initial parts of the text.\n",
      "- A proposed solution is to develop a summarization method that takes into account the structure of the text and provides the text's main topics, addressing the limitations of existing methods. This method can be easily implemented in Python with several tweaks of the existing method.\n"
     ]
    }
   ],
   "source": [
    "text_2 = '''LLMs like GPT-4 have taken the world by storm, and one of the tasks that generative text models are particularly good at is summarization of long texts such as books or podcast transcripts. However, the conventional method of getting LLMs to summarize long texts is actually fundamentally flawed. In this post, I will tell you about the problems with existing summarization methods, and present a better summarization method that actually takes into account the structure of the text! Even better, this method will also give us the text’s main topics — killing two birds with one stone!I will walk you through how you can easily implement this in Python, with just several tweaks of the existing method. This is the method that we use at Podsmart, our newly-launched AI-powered podcast summarizer app that helps busy intellectuals save hours of listening.Problems with existing solutionsThe canonical method to summarize long texts is by recursive summarization, in which the long text is split equally into shorter chunks which can fit inside the LLM’s context window. Each chunk is summarized, and the summaries are concatenated together to and then passed through GPT-3 to be further summarized. This process is repeated until one obtains a final summary of desired length.However, the major downside is that existing implementations e.g. LangChain’s summarize chain using map_reduce, split the text into chunks with no regard for the logical and structural flow of the text.For example, if the article is 1000 words long, a chunk size of 200 would mean that we would get 5 chunks. What if the author has several main points, the first of which takes up the first 250 words? The last 50 words would be placed into the second chunk with text from the author’s next point, and passing this chunk through GPT-3’s summarizer would lead to potentially important information from the first point being omitted. Also, some key points may be longer than others, and there is no way of knowing this a priori.Another method is the ‘refine’ method, which passes every chunk of text, along with a summary from previous chunks, through the LLM, which progressively refines the summary as it sees more of the text (see the prompt here). However, the sequential nature of the process means that it cannot be parallelized and takes linear time, far longer than a recursive method which takes logarithmic time. Additionally, intuition suggests that the meaning from the initial parts will be overrepresented in the final summary. For podcast transcripts where the first minutes are advertisements completely irrelevant to the rest of the podcast, this is a stumbling block. Hence, this method is hence not widely used.'''\n",
    "\n",
    "prompt = '''<s> [INST]\n",
    "Write a concise summary for text enclosed in triple backticks (```)\n",
    "- Output the summary in bullet points.\n",
    "- Each point should contain no more than 2 sentences.\n",
    "- Keep the summary concise.- Points should not contain redundant information.\n",
    "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
    "- Summary should be grammatically correct.\n",
    "\n",
    "```\n",
    "LLMs like GPT-4 have taken the world by storm, and one of the tasks that generative text models are particularly good at is summarization of long texts such as books or podcast transcripts. However, the conventional method of getting LLMs to summarize long texts is actually fundamentally flawed. In this post, I will tell you about the problems with existing summarization methods, and present a better summarization method that actually takes into account the structure of the text! Even better, this method will also give us the text’s main topics — killing two birds with one stone!I will walk you through how you can easily implement this in Python, with just several tweaks of the existing method. This is the method that we use at Podsmart, our newly-launched AI-powered podcast summarizer app that helps busy intellectuals save hours of listening.Problems with existing solutionsThe canonical method to summarize long texts is by recursive summarization, in which the long text is split equally into shorter chunks which can fit inside the LLM’s context window. Each chunk is summarized, and the summaries are concatenated together to and then passed through GPT-3 to be further summarized. This process is repeated until one obtains a final summary of desired length.However, the major downside is that existing implementations e.g. LangChain’s summarize chain using map_reduce, split the text into chunks with no regard for the logical and structural flow of the text.For example, if the article is 1000 words long, a chunk size of 200 would mean that we would get 5 chunks. What if the author has several main points, the first of which takes up the first 250 words? The last 50 words would be placed into the second chunk with text from the author’s next point, and passing this chunk through GPT-3’s summarizer would lead to potentially important information from the first point being omitted. Also, some key points may be longer than others, and there is no way of knowing this a priori.Another method is the ‘refine’ method, which passes every chunk of text, along with a summary from previous chunks, through the LLM, which progressively refines the summary as it sees more of the text (see the prompt here). However, the sequential nature of the process means that it cannot be parallelized and takes linear time, far longer than a recursive method which takes logarithmic time. Additionally, intuition suggests that the meaning from the initial parts will be overrepresented in the final summary. For podcast transcripts where the first minutes are advertisements completely irrelevant to the rest of the podcast, this is a stumbling block. Hence, this method is hence not widely used.\n",
    "```\n",
    "\n",
    "Return your answer in the following format:\n",
    "Title: <title>\n",
    "Summary:\n",
    "- <summary point 1>\n",
    "- <summary point 2>\n",
    "- <summary point n>\n",
    "e.g.\n",
    "Title: Why Artificial Intelligence is Good\n",
    "Summary:\n",
    "- AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "---\n",
    "TITLE AND CONCISE SUMMARY:\n",
    "[/INST]'''\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "mistral_res = together_client.chat.completions.create(\n",
    "  model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "  messages=[{'role': 'user', 'content': prompt}],\n",
    "  temperature=0.4,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(mistral_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Superior Summarization Method for Long Texts Using LLMs\n",
      "Summary: [\"The conventional recursive summarization method for long texts using LLMs, such as GPT-3, splits texts into equal chunks without considering the text's logical and structural flow, leading to potential loss of important information.\", \"The 'refine' method, which progressively refines the summary as it sees more of the text, cannot be parallelized and takes linear time, making it less efficient. Additionally, it may overrepresent the meaning from the initial parts of the text.\", 'A proposed solution is to develop a summarization method th ...']\n"
     ]
    }
   ],
   "source": [
    "# title and summary parser\n",
    "import re\n",
    "\n",
    "text = \"\"\"\n",
    "Title: A Superior Summarization Method for Long Texts Using LLMs\n",
    "\n",
    "Summary:\n",
    "- The conventional recursive summarization method for long texts using LLMs, such as GPT-3, splits texts into equal chunks without considering the text's logical and structural flow, leading to potential loss of important information.\n",
    "- The 'refine' method, which progressively refines the summary as it sees more of the text, cannot be parallelized and takes linear time, making it less efficient. Additionally, it may overrepresent the meaning from the initial parts of the text.\n",
    "- A proposed solution is to develop a summarization method th ...\n",
    "\"\"\"\n",
    "\n",
    "title_pattern = r\"Title: (.*)\\n\"\n",
    "summary_pattern = r\"Summary:\\n((?:- .*\\n)*)\"\n",
    "\n",
    "title = re.search(title_pattern, text).group(1)\n",
    "summary = re.findall(summary_pattern, text)[0].split('\\n')\n",
    "summary = [item.strip('- ') for item in summary if item]\n",
    "\n",
    "print(\"Title:\", title)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input:\n",
      "---\n",
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "```\n",
      "Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally -- wisely -- left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn't let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they're all still there, and Larry is not. Micro-managing isn't that third thing that Amazon does better than us, by the way. I mean, yeah, they micro-manage really well, but I wouldn't list it as a strength or anything. I'm just trying to set the context here, to help you understand what happened. We're talking about a guy who in all seriousness has said on many public occasions that people should be paying him to work at Amazon. He hands out little yellow stickies with his name on them, reminding people \"who runs the company\" when they disagree with him. The guy is a regular... well, Steve Jobs, I guess. Except without the fashion or design sense. Bezos is super smart; don't get me wrong. He just makes ordinary control freaks look like stoned hippies.\n",
      "```\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "---\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "\n",
      "---\n",
      "Output:\n",
      "---\n",
      " Title: Jeff Bezos' Micro-Managing Style\n",
      "\n",
      "Summary:\n",
      "\n",
      "* Jeff Bezos is known for his extreme micro-managing style, which includes closely overseeing every aspect of Amazon's retail site, including the placement of every pixel.\n",
      "* Despite hiring renowned experts like Larry Tesler, Bezos ignores their advice and insists on maintaining control over even the smallest details.\n"
     ]
    }
   ],
   "source": [
    "prompt = map_prompt_template.format(text=text)\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "llama2_70_res = together_client.chat.completions.create(\n",
    "  model=\"togethercomputer/llama-2-70b-chat\",\n",
    "  messages=[{'role': 'user', 'content': map_prompt_template.format(text=text)}],\n",
    "  temperature=0.4,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(llama2_70_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input:\n",
      "---\n",
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "```\n",
      "Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally -- wisely -- left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn't let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they're all still there, and Larry is not. Micro-managing isn't that third thing that Amazon does better than us, by the way. I mean, yeah, they micro-manage really well, but I wouldn't list it as a strength or anything. I'm just trying to set the context here, to help you understand what happened. We're talking about a guy who in all seriousness has said on many public occasions that people should be paying him to work at Amazon. He hands out little yellow stickies with his name on them, reminding people \"who runs the company\" when they disagree with him. The guy is a regular... well, Steve Jobs, I guess. Except without the fashion or design sense. Bezos is super smart; don't get me wrong. He just makes ordinary control freaks look like stoned hippies.\n",
      "```\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "---\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "\n",
      "---\n",
      "Output:\n",
      "---\n",
      "Title: Jeff Bezos' Micro-Management Style\n",
      "Summary:\n",
      "- Jeff Bezos is known for his micro-management style.\n",
      "- He micro-manages every single pixel of Amazon's retail site.\n",
      "- He hired Larry Tesler, a renowned human-computer interaction expert, but ignored his advice for three years.\n",
      "- Bezos is a control freak who believes people should pay him to work at Amazon.\n",
      "- He hands out stickies with his name on them to remind people who runs the company.\n",
      "- Bezos is super smart but makes ordinary control freaks look like stoned hippies.\n"
     ]
    }
   ],
   "source": [
    "prompt = map_prompt_template.format(text=text)\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "llama2_70_res = together_client.chat.completions.create(\n",
    "  model=\"NousResearch/Nous-Hermes-Llama2-70b\",\n",
    "  messages=[{'role': 'user', 'content': map_prompt_template.format(text=text)}],\n",
    "  temperature=0.4,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(llama2_70_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input:\n",
      "---\n",
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "```\n",
      "Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally -- wisely -- left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn't let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they're all still there, and Larry is not. Micro-managing isn't that third thing that Amazon does better than us, by the way. I mean, yeah, they micro-manage really well, but I wouldn't list it as a strength or anything. I'm just trying to set the context here, to help you understand what happened. We're talking about a guy who in all seriousness has said on many public occasions that people should be paying him to work at Amazon. He hands out little yellow stickies with his name on them, reminding people \"who runs the company\" when they disagree with him. The guy is a regular... well, Steve Jobs, I guess. Except without the fashion or design sense. Bezos is super smart; don't get me wrong. He just makes ordinary control freaks look like stoned hippies.\n",
      "```\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "---\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "\n",
      "---\n",
      "Output:\n",
      "---\n",
      "Title: Jeff Bezos' Micro-Management Style\n",
      "Summary:\n",
      "- Jeff Bezos is an infamous micro-manager who micro-manages every single pixel of Amazon's retail site.\n",
      "- He hired Larry Tesler, Apple's Chief Scientist, and ignored his advice for three years until Larry left the company.\n",
      "- Bezos is super smart but makes ordinary control freaks look like stoned hippies.\n"
     ]
    }
   ],
   "source": [
    "prompt = map_prompt_template.format(text=text)\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "llama2_70_res = together_client.chat.completions.create(\n",
    "  model=\"NousResearch/Nous-Hermes-Llama2-70b\",\n",
    "  messages=[{'role': 'user', 'content': map_prompt_template.format(text=text)}],\n",
    "  temperature=0,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(llama2_70_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above one looks good. Trying with different text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input:\n",
      "---\n",
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "```\n",
      "LLMs like GPT-4 have taken the world by storm, and one of the tasks that generative text models are particularly good at is summarization of long texts such as books or podcast transcripts. However, the conventional method of getting LLMs to summarize long texts is actually fundamentally flawed. In this post, I will tell you about the problems with existing summarization methods, and present a better summarization method that actually takes into account the structure of the text! Even better, this method will also give us the text’s main topics — killing two birds with one stone!I will walk you through how you can easily implement this in Python, with just several tweaks of the existing method. This is the method that we use at Podsmart, our newly-launched AI-powered podcast summarizer app that helps busy intellectuals save hours of listening.Problems with existing solutionsThe canonical method to summarize long texts is by recursive summarization, in which the long text is split equally into shorter chunks which can fit inside the LLM’s context window. Each chunk is summarized, and the summaries are concatenated together to and then passed through GPT-3 to be further summarized. This process is repeated until one obtains a final summary of desired length.However, the major downside is that existing implementations e.g. LangChain’s summarize chain using map_reduce, split the text into chunks with no regard for the logical and structural flow of the text.For example, if the article is 1000 words long, a chunk size of 200 would mean that we would get 5 chunks. What if the author has several main points, the first of which takes up the first 250 words? The last 50 words would be placed into the second chunk with text from the author’s next point, and passing this chunk through GPT-3’s summarizer would lead to potentially important information from the first point being omitted. Also, some key points may be longer than others, and there is no way of knowing this a priori.Another method is the ‘refine’ method, which passes every chunk of text, along with a summary from previous chunks, through the LLM, which progressively refines the summary as it sees more of the text (see the prompt here). However, the sequential nature of the process means that it cannot be parallelized and takes linear time, far longer than a recursive method which takes logarithmic time. Additionally, intuition suggests that the meaning from the initial parts will be overrepresented in the final summary. For podcast transcripts where the first minutes are advertisements completely irrelevant to the rest of the podcast, this is a stumbling block. Hence, this method is hence not widely used.\n",
      "```\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "---\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "\n",
      "---\n",
      "Output:\n",
      "---\n",
      "Title: Improving Text Summarization with LLMs\n",
      "Summary:\n",
      "- LLMs like GPT-4 are good at summarizing long texts.\n",
      "- Existing summarization methods have flaws, such as splitting text into chunks without considering structure.\n",
      "- A better method is recursive summarization, which takes into account text structure and gives main topics.\n",
      "- This method can be implemented in Python with tweaks to existing methods.\n"
     ]
    }
   ],
   "source": [
    "text_2 = '''LLMs like GPT-4 have taken the world by storm, and one of the tasks that generative text models are particularly good at is summarization of long texts such as books or podcast transcripts. However, the conventional method of getting LLMs to summarize long texts is actually fundamentally flawed. In this post, I will tell you about the problems with existing summarization methods, and present a better summarization method that actually takes into account the structure of the text! Even better, this method will also give us the text’s main topics — killing two birds with one stone!I will walk you through how you can easily implement this in Python, with just several tweaks of the existing method. This is the method that we use at Podsmart, our newly-launched AI-powered podcast summarizer app that helps busy intellectuals save hours of listening.Problems with existing solutionsThe canonical method to summarize long texts is by recursive summarization, in which the long text is split equally into shorter chunks which can fit inside the LLM’s context window. Each chunk is summarized, and the summaries are concatenated together to and then passed through GPT-3 to be further summarized. This process is repeated until one obtains a final summary of desired length.However, the major downside is that existing implementations e.g. LangChain’s summarize chain using map_reduce, split the text into chunks with no regard for the logical and structural flow of the text.For example, if the article is 1000 words long, a chunk size of 200 would mean that we would get 5 chunks. What if the author has several main points, the first of which takes up the first 250 words? The last 50 words would be placed into the second chunk with text from the author’s next point, and passing this chunk through GPT-3’s summarizer would lead to potentially important information from the first point being omitted. Also, some key points may be longer than others, and there is no way of knowing this a priori.Another method is the ‘refine’ method, which passes every chunk of text, along with a summary from previous chunks, through the LLM, which progressively refines the summary as it sees more of the text (see the prompt here). However, the sequential nature of the process means that it cannot be parallelized and takes linear time, far longer than a recursive method which takes logarithmic time. Additionally, intuition suggests that the meaning from the initial parts will be overrepresented in the final summary. For podcast transcripts where the first minutes are advertisements completely irrelevant to the rest of the podcast, this is a stumbling block. Hence, this method is hence not widely used.'''\n",
    "\n",
    "prompt = map_prompt_template.format(text=text_2)\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "llama2_70_res = together_client.chat.completions.create(\n",
    "  model=\"NousResearch/Nous-Hermes-Llama2-70b\",\n",
    "  messages=[{'role': 'user', 'content': prompt}],\n",
    "  temperature=0,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(llama2_70_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- It summarized that starting text good.\n",
    "- It messed up saying 'recursive summarization is better option' which is exactly opposite of what the author suggests, but it is in the same sentence as the what we are going to achieve part, which is correct in the output.\n",
    "- It forgot to summarize the last part of the text.\n",
    "\n",
    "Conclusion:\n",
    "- Shorten input\n",
    "- First stage is to just remove fluff, and then cluster similar text together.\n",
    "- Second stage seems to be the most important part, which is to summarize the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input:\n",
      "---\n",
      "### Instruction:\n",
      "\n",
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "### Input:\n",
      "```\n",
      "LLMs like GPT-4 have taken the world by storm, and one of the tasks that generative text models are particularly good at is summarization of long texts such as books or podcast transcripts. However, the conventional method of getting LLMs to summarize long texts is actually fundamentally flawed. In this post, I will tell you about the problems with existing summarization methods, and present a better summarization method that actually takes into account the structure of the text! Even better, this method will also give us the text’s main topics — killing two birds with one stone!I will walk you through how you can easily implement this in Python, with just several tweaks of the existing method. This is the method that we use at Podsmart, our newly-launched AI-powered podcast summarizer app that helps busy intellectuals save hours of listening.Problems with existing solutionsThe canonical method to summarize long texts is by recursive summarization, in which the long text is split equally into shorter chunks which can fit inside the LLM’s context window. Each chunk is summarized, and the summaries are concatenated together to and then passed through GPT-3 to be further summarized. This process is repeated until one obtains a final summary of desired length.However, the major downside is that existing implementations e.g. LangChain’s summarize chain using map_reduce, split the text into chunks with no regard for the logical and structural flow of the text.For example, if the article is 1000 words long, a chunk size of 200 would mean that we would get 5 chunks. What if the author has several main points, the first of which takes up the first 250 words? The last 50 words would be placed into the second chunk with text from the author’s next point, and passing this chunk through GPT-3’s summarizer would lead to potentially important information from the first point being omitted. Also, some key points may be longer than others, and there is no way of knowing this a priori.Another method is the ‘refine’ method, which passes every chunk of text, along with a summary from previous chunks, through the LLM, which progressively refines the summary as it sees more of the text (see the prompt here). However, the sequential nature of the process means that it cannot be parallelized and takes linear time, far longer than a recursive method which takes logarithmic time. Additionally, intuition suggests that the meaning from the initial parts will be overrepresented in the final summary. For podcast transcripts where the first minutes are advertisements completely irrelevant to the rest of the podcast, this is a stumbling block. Hence, this method is hence not widely used.\n",
      "```\n",
      "\n",
      "### Response:\n",
      "\n",
      "---\n",
      "Output:\n",
      "---\n",
      "Title: Problems with Existing Summarization Methods\n",
      "Summary:\n",
      "- Existing summarization methods split long texts into chunks without considering the logical and structural flow of the text.\n",
      "- This can lead to important information being omitted or overrepresented in the final summary.\n",
      "- The recursive method is faster than the refine method, but cannot be parallelized.\n",
      "- The refine method is slower and cannot be parallelized.\n"
     ]
    }
   ],
   "source": [
    "custom_prompt = '''### Instruction:\n",
    "\n",
    "Write a concise summary for text enclosed in triple backticks (```)\n",
    "- Output the summary in bullet points.\n",
    "- Each point should contain no more than 2 sentences.\n",
    "- Keep the summary concise.- Points should not contain redundant information.\n",
    "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
    "- Summary should be grammatically correct.\n",
    "\n",
    "Return your answer in the following format:\n",
    "Title: <title>\n",
    "Summary:\n",
    "- <summary point 1>\n",
    "- <summary point 2>\n",
    "- <summary point n>\n",
    "e.g.\n",
    "Title: Why Artificial Intelligence is Good\n",
    "Summary:\n",
    "- AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "### Input:\n",
    "```\n",
    "LLMs like GPT-4 have taken the world by storm, and one of the tasks that generative text models are particularly good at is summarization of long texts such as books or podcast transcripts. However, the conventional method of getting LLMs to summarize long texts is actually fundamentally flawed. In this post, I will tell you about the problems with existing summarization methods, and present a better summarization method that actually takes into account the structure of the text! Even better, this method will also give us the text’s main topics — killing two birds with one stone!I will walk you through how you can easily implement this in Python, with just several tweaks of the existing method. This is the method that we use at Podsmart, our newly-launched AI-powered podcast summarizer app that helps busy intellectuals save hours of listening.Problems with existing solutionsThe canonical method to summarize long texts is by recursive summarization, in which the long text is split equally into shorter chunks which can fit inside the LLM’s context window. Each chunk is summarized, and the summaries are concatenated together to and then passed through GPT-3 to be further summarized. This process is repeated until one obtains a final summary of desired length.However, the major downside is that existing implementations e.g. LangChain’s summarize chain using map_reduce, split the text into chunks with no regard for the logical and structural flow of the text.For example, if the article is 1000 words long, a chunk size of 200 would mean that we would get 5 chunks. What if the author has several main points, the first of which takes up the first 250 words? The last 50 words would be placed into the second chunk with text from the author’s next point, and passing this chunk through GPT-3’s summarizer would lead to potentially important information from the first point being omitted. Also, some key points may be longer than others, and there is no way of knowing this a priori.Another method is the ‘refine’ method, which passes every chunk of text, along with a summary from previous chunks, through the LLM, which progressively refines the summary as it sees more of the text (see the prompt here). However, the sequential nature of the process means that it cannot be parallelized and takes linear time, far longer than a recursive method which takes logarithmic time. Additionally, intuition suggests that the meaning from the initial parts will be overrepresented in the final summary. For podcast transcripts where the first minutes are advertisements completely irrelevant to the rest of the podcast, this is a stumbling block. Hence, this method is hence not widely used.\n",
    "```\n",
    "\n",
    "### Response:\n",
    "'''\n",
    "\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(custom_prompt)\n",
    "\n",
    "llama2_70_res = together_client.chat.completions.create(\n",
    "  model=\"NousResearch/Nous-Hermes-Llama2-70b\",\n",
    "  messages=[{'role': 'user', 'content': custom_prompt}],\n",
    "  temperature=0,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(llama2_70_res.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input:\n",
      "---\n",
      "Write a concise summary for text enclosed in triple backticks (```)\n",
      "- Output the summary in bullet points.\n",
      "- Each point should contain no more than 2 sentences.\n",
      "- Keep the summary concise.- Points should not contain redundant information.\n",
      "- Do not use pronouns. Resolve coreference and use proper nouns or common nouns.\n",
      "- Summary should be grammatically correct.\n",
      "\n",
      "```\n",
      "Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon's retail site. He hired Larry Tesler, Apple's Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally -- wisely -- left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn't let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they're all still there, and Larry is not. Micro-managing isn't that third thing that Amazon does better than us, by the way. I mean, yeah, they micro-manage really well, but I wouldn't list it as a strength or anything. I'm just trying to set the context here, to help you understand what happened. We're talking about a guy who in all seriousness has said on many public occasions that people should be paying him to work at Amazon. He hands out little yellow stickies with his name on them, reminding people \"who runs the company\" when they disagree with him. The guy is a regular... well, Steve Jobs, I guess. Except without the fashion or design sense. Bezos is super smart; don't get me wrong. He just makes ordinary control freaks look like stoned hippies.\n",
      "```\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title: <title>\n",
      "Summary:\n",
      "- <summary point 1>\n",
      "- <summary point 2>\n",
      "- <summary point n>\n",
      "e.g.\n",
      "Title: Why Artificial Intelligence is Good\n",
      "Summary:\n",
      "- AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "---\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "\n",
      "---\n",
      "Output:\n",
      "---\n",
      "Title: Jeff Bezos' Micro-Management at Amazon\n",
      "Summary:\n",
      "- Jeff Bezos is known for his micro-management of Amazon's retail site.\n",
      "- He ignored Larry Tesler's usability studies and advice, leading to Tesler's departure.\n",
      "- Bezos' control over the website is compared to his management style, which is described as extreme.\n"
     ]
    }
   ],
   "source": [
    "prompt = map_prompt_template.format(text=text)\n",
    "print('---')\n",
    "print('Input:')\n",
    "print('---')\n",
    "print(prompt)\n",
    "\n",
    "llama2_70_res = together_client.chat.completions.create(\n",
    "  model=\"NousResearch/Nous-Hermes-2-Yi-34B\",\n",
    "  messages=[{'role': 'user', 'content': map_prompt_template.format(text=text)}],\n",
    "  temperature=0.4,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.6,\n",
    ")\n",
    "\n",
    "print('---')\n",
    "print('Output:')\n",
    "print('---')\n",
    "print(llama2_70_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
